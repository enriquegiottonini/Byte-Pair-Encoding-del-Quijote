{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Procesamiento de Lenguaje Natural\n",
    "\n",
    "*MINI-TASK \\#1* \n",
    "\n",
    "# ***Byte-Pair-Encoding del Quijote***\n",
    "\n",
    "### **Equipo:**\n",
    "\n",
    "- Giottonini Herrera Enrique Alejandro\n",
    "- Burruel Durán Luis Andrés\n",
    "- Jorge Andres Rascon Acuña\n",
    "- Villalba Miranda Jesús Abraham\n",
    "\n",
    "**Fuentes**\n",
    "* [Byte Pair Encoding (Lei Mao) ](https://leimao.github.io/blog/Byte-Pair-Encoding/)\n",
    "* [Byte-Pair Encoding: Subword-based tokenization algorithm (Chetna Khanna) ](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducción**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **byte pair encoding (BPE)** es una técnica sencilla de comprimir datos que consiste en reemplazar la combinación de bytes más frecuente en los datos con un byte que no aparece en ellos.\n",
    "\n",
    "El objetivo es que al terminar de procesar los datos las palabras más comunes estén representadas en el vocabulario como un solo token, mientras que las palabras raras se dividen en dos o más tokens de subpalabras, lo cual coincide con lo que hace un algoritmo de **subword-based tokenization**.\n",
    "\n",
    "Los datos que se utilizaran para esta tarea sera _\"Don Quijote de la Mancha\"_ de _Miguel de Cervantes Saavedra_ en version EBOOK transcrito por _The Project Gutenberg_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendiendo de nuestro dataset\n",
    "Antes de armar el algoritmo de **BPE** primero explicaremos las funciones que necesitamos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerias que necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vocab per words**\n",
    "\n",
    "La función get_vocab se encarga de obtener las palabras contenidas en el vocabulario del corpus y la frecuencia de estas. Para esto, la función recibe el parámetro filename, esto es, el archivo a analizar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    vocab = collections.defaultdict(int) #Define primero un diccionario vacío llamado \"vocab\"\n",
    "    #A continuación, se abre un archivo con la función open, donde la regresa como archivo objeto.\n",
    "    #Se utiliza como parámetros 'filename', 'r' que indica que se va a leer el archivo (read) y \n",
    "    #Por otro lado, utf-8 es un formato de codificación de caracteres Unicode e ISO 10646.\n",
    "    with open(filename, 'r', encoding='utf-8') as fhand:  \n",
    "        for line in fhand: #lee cada linea (line)\n",
    "            words = line.strip().split() #se quitan espacios en blanco y se separan palabras\n",
    "\n",
    "            #Finalmente, incluye cada palabra en el diccionario y su frecuencia, anexando al final </w>\n",
    "            for word in words:\n",
    "                word = re.sub(\"[.,;\\-:!¡¿?]\", '', word ) #Quita caracteres especiales\n",
    "                word = word.lower() #Transforma a minúsculas\n",
    "                vocab[' '.join(list(word)) + ' </w>'] += 1 \n",
    "                \n",
    "    return vocab #Regresa el diccionario \"vocab\" con el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocabulario = get_vocab('corpus/extract.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario inicial:\n",
      "y o </w>: 1\n",
      "j u a n </w>: 1\n",
      "g a l l o </w>: 1\n",
      "d e </w>: 8\n",
      "a n d r a d a </w>: 1\n",
      "e s c r i b a n o </w>: 1\n",
      "c á m a r a </w>: 1\n",
      "d e l </w>: 4\n",
      "r e y </w>: 1\n",
      "n u e s t r o </w>: 1\n",
      "s e ñ o r </w>: 1\n",
      "l o s </w>: 2\n",
      "q u e </w>: 7\n",
      "r e s i d e n </w>: 1\n",
      "e n </w>: 4\n",
      "s u </w>: 1\n",
      "c o n s e j o </w>: 1\n",
      "c e r t i f i c o </w>: 1\n",
      "y </w>: 11\n",
      "d o y </w>: 1\n",
      "f e </w>: 1\n",
      "h a b i e n d o </w>: 1\n",
      "v i s t o </w>: 1\n",
      "p o r </w>: 2\n",
      "s e ñ o r e s </w>: 1\n",
      "d é l </w>: 1\n",
      "u n </w>: 1\n",
      "l i b r o </w>: 4\n",
      "i n t i t u l a d o </w>: 1\n",
      "e l </w>: 3\n",
      "i n g e n i o s o </w>: 1\n",
      "h i d a l g o </w>: 1\n",
      "l a </w>: 2\n",
      "m a n c h a </w>: 1\n",
      "c o m p u e s t o </w>: 1\n",
      "m i g u e l </w>: 1\n",
      "c e r v a n t e s </w>: 1\n",
      "s a a v e d r a </w>: 1\n",
      "t a s a r o n </w>: 1\n",
      "c a d a </w>: 1\n",
      "p l i e g o </w>: 1\n",
      "d i c h o </w>: 4\n",
      "a </w>: 3\n",
      "t r e s </w>: 2\n",
      "m a r a v e d í s </w>: 2\n",
      "m e d i o </w>: 2\n",
      "c u a l </w>: 1\n",
      "t i e n e </w>: 1\n",
      "o c h e n t a </w>: 1\n",
      "p l i e g o s </w>: 1\n",
      "a l </w>: 2\n",
      "p r e c i o </w>: 2\n",
      "m o n t a </w>: 1\n",
      "d o c i e n t o s </w>: 1\n",
      "n o v e n t a </w>: 1\n",
      "s e </w>: 4\n",
      "h a </w>: 1\n",
      "v e n d e r </w>: 3\n",
      "p a p e l </w>: 1\n",
      "d i e r o n </w>: 1\n",
      "l i c e n c i a </w>: 1\n",
      "p a r a </w>: 2\n",
      "e s t e </w>: 1\n",
      "p u e d a </w>: 2\n",
      "m a n d a r o n </w>: 1\n",
      "e s t a </w>: 1\n",
      "t a s a </w>: 1\n",
      "p o n g a </w>: 1\n",
      "p r i n c i p i o </w>: 1\n",
      "n o </w>: 1\n",
      "s i n </w>: 1\n",
      "e l l a </w>: 1\n",
      "d e l l o </w>: 1\n",
      "c o n s t e </w>: 1\n",
      "d i </w>: 1\n",
      "p r e s e n t e </w>: 1\n",
      "v a l l a d o l i d </w>: 1\n",
      "v e i n t e </w>: 1\n",
      "d í a s </w>: 1\n",
      "m e s </w>: 1\n",
      "d e c i e m b r e </w>: 1\n",
      "m i l </w>: 1\n",
      "s e i s c i e n t o s </w>: 1\n",
      "c u a t r o </w>: 1\n",
      "a ñ o s </w>: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulario inicial:\")\n",
    "for word, freq in vocabulario.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Obteniendo los pares**\n",
    "\n",
    "La función get_stats tiene como parámetro el diccionario vocab generado con la función get_vocab, y devuelve otro diccionario \"pairs\" con la frecuencia de los pares de tokens más recurrentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) #Define primero un diccionario vacío llamado \"pairs\"\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split() #Separa la palabra en tokens\n",
    "        for i in range(len(symbols)-1): #Recorre todos los tokens de la palabra, excluyendo </w>\n",
    "            #Se suman las veces que se repite una palabra a la frecuencia del par de tokens\n",
    "            pairs[symbols[i],symbols[i+1]] += freq \n",
    "    return pairs #Regresa el diccionario con los pares de tokens mas recurrentes y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_stats(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('o', '</w>'), 29)\n",
      "(('e', '</w>'), 26)\n",
      "(('a', '</w>'), 23)\n",
      "(('d', 'e'), 18)\n",
      "(('e', 'n'), 17)\n",
      "(('l', '</w>'), 14)\n",
      "(('s', '</w>'), 14)\n",
      "(('y', '</w>'), 13)\n",
      "(('e', 's'), 12)\n",
      "(('u', 'e'), 12)\n"
     ]
    }
   ],
   "source": [
    "sorted_pairs = sorted(get_stats(vocabulario).items(), key=lambda x:x[1],reverse=True)\n",
    "for i in range(10):\n",
    "    print(sorted_pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El par de caracteres consecutivos más frecuentes es: ('o', '</w>')\n"
     ]
    }
   ],
   "source": [
    "best = max(pairs, key=pairs.get)\n",
    "print(f\"El par de caracteres consecutivos más frecuentes es: {best}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merge**\n",
    "La función `merge_vocab` es la encargada de actualizar el vocabulario al mezclar el par de caracteres más frecuente. \n",
    "\n",
    "Esta función recibe:\n",
    "1. **pair**: par más frecuente en el vocabulario\n",
    "2. **v_in**: el vocabulario.\n",
    "\n",
    "La función regresa el vocabulario actualizado despues de unir los caracteres y actualizar la frequencia de los tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' ' .join(pair)) # (\"w1\", \"w2\") -> \"w1 w2\"\n",
    "    # Expresión regular utilizada para buscar el par más frecuente de caracteres\n",
    "    # en el vocabucario\"\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función utilizamos el par más frecuente de caracteres de `vocabulario`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El par mas frequente: ('o', '</w>')\n",
      "Nuevo vocabulario:\n",
      "\n",
      "y o</w>: 1\n",
      "j u a n </w>: 1\n",
      "g a l l o</w>: 1\n",
      "d e </w>: 8\n",
      "a n d r a d a </w>: 1\n",
      "e s c r i b a n o</w>: 1\n",
      "c á m a r a </w>: 1\n",
      "d e l </w>: 4\n",
      "r e y </w>: 1\n",
      "n u e s t r o</w>: 1\n",
      "s e ñ o r </w>: 1\n",
      "l o s </w>: 2\n",
      "q u e </w>: 7\n",
      "r e s i d e n </w>: 1\n",
      "e n </w>: 4\n",
      "s u </w>: 1\n",
      "c o n s e j o</w>: 1\n",
      "c e r t i f i c o</w>: 1\n",
      "y </w>: 11\n",
      "d o y </w>: 1\n",
      "f e </w>: 1\n",
      "h a b i e n d o</w>: 1\n",
      "v i s t o</w>: 1\n",
      "p o r </w>: 2\n",
      "s e ñ o r e s </w>: 1\n",
      "d é l </w>: 1\n",
      "u n </w>: 1\n",
      "l i b r o</w>: 4\n",
      "i n t i t u l a d o</w>: 1\n",
      "e l </w>: 3\n",
      "i n g e n i o s o</w>: 1\n",
      "h i d a l g o</w>: 1\n",
      "l a </w>: 2\n",
      "m a n c h a </w>: 1\n",
      "c o m p u e s t o</w>: 1\n",
      "m i g u e l </w>: 1\n",
      "c e r v a n t e s </w>: 1\n",
      "s a a v e d r a </w>: 1\n",
      "t a s a r o n </w>: 1\n",
      "c a d a </w>: 1\n",
      "p l i e g o</w>: 1\n",
      "d i c h o</w>: 4\n",
      "a </w>: 3\n",
      "t r e s </w>: 2\n",
      "m a r a v e d í s </w>: 2\n",
      "m e d i o</w>: 2\n",
      "c u a l </w>: 1\n",
      "t i e n e </w>: 1\n",
      "o c h e n t a </w>: 1\n",
      "p l i e g o s </w>: 1\n",
      "a l </w>: 2\n",
      "p r e c i o</w>: 2\n",
      "m o n t a </w>: 1\n",
      "d o c i e n t o s </w>: 1\n",
      "n o v e n t a </w>: 1\n",
      "s e </w>: 4\n",
      "h a </w>: 1\n",
      "v e n d e r </w>: 3\n",
      "p a p e l </w>: 1\n",
      "d i e r o n </w>: 1\n",
      "l i c e n c i a </w>: 1\n",
      "p a r a </w>: 2\n",
      "e s t e </w>: 1\n",
      "p u e d a </w>: 2\n",
      "m a n d a r o n </w>: 1\n",
      "e s t a </w>: 1\n",
      "t a s a </w>: 1\n",
      "p o n g a </w>: 1\n",
      "p r i n c i p i o</w>: 1\n",
      "n o</w>: 1\n",
      "s i n </w>: 1\n",
      "e l l a </w>: 1\n",
      "d e l l o</w>: 1\n",
      "c o n s t e </w>: 1\n",
      "d i </w>: 1\n",
      "p r e s e n t e </w>: 1\n",
      "v a l l a d o l i d </w>: 1\n",
      "v e i n t e </w>: 1\n",
      "d í a s </w>: 1\n",
      "m e s </w>: 1\n",
      "d e c i e m b r e </w>: 1\n",
      "m i l </w>: 1\n",
      "s e i s c i e n t o s </w>: 1\n",
      "c u a t r o</w>: 1\n",
      "a ñ o s </w>: 1\n"
     ]
    }
   ],
   "source": [
    "new_vocab = merge_vocab(best, vocabulario)\n",
    "print(f\"El par mas frequente: {best}\")\n",
    "print(\"Nuevo vocabulario:\\n\")\n",
    "for word, freq in new_vocab.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get tokens**\n",
    "La función `get_tokens` regresa los los tipos y su frecuencia del vocabulario del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tipos de nuestro vocabulario serian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens(vocabulario)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora elaboramos una función auxiliar para guardar los tipos y su frecuencia en un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(tokens, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Tipo, Frecuencia\\n\")\n",
    "        for token, freq in tokens.items():\n",
    "            file.write(f\"{token}, {freq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(tokens, 'vocab.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Armando el algoritmo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_pair_encoding(vocab, iter):\n",
    "    vocab_aux = vocab\n",
    "    for i in range(iter):\n",
    "        pairs = get_stats(vocab_aux)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # print(f\"Iter {i+1}:\")\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab_aux = merge_vocab(best, vocab_aux)\n",
    "        tokens = get_tokens(vocab_aux)\n",
    "        # print(f\"Mejor par {best} \\nNúmero de tokens: {len(tokens)}\")\n",
    "        # print('============')\n",
    "    return vocab_aux"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aplicandolo al Quijote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = get_vocab('corpus/quijote.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(get_tokens(vocabulario), 'outputs/pretokenization/vocabulario_inicial.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Número de iteraciones del algoritmo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = byte_pair_encoding(vocabulario,num_merges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guardamos nuestro vocabulario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(get_tokens(vocabulario), 'outputs/pretokenization/vocab_quijote_1000.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoding and Decoding**\n",
    "\n",
    "El ***encoding*** consiste en que dado un vocabulario de tipos $V$ y un corpus/texto $C$ podemos utilizar una función de encoding $E: V'\\times C' \\to T$ que regresa una lista/secuencia de tokens reconocidos $T$ sobre un corpus $C'$ preprocesado y un vocabulario extendido $V'$ que contiene tokens para casos especiales como cadenas no reconocidas (OOV). De esta manera podemos darle una estructura utilizable al texto. De forma inversa, el ***decoding*** consiste en convertir una lista de tokens en texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función que pueda leer un diccionario en algun archivo de texto y asegurarse de que los tipos estén ordenados y de filtrar aquellos tokens que tuvieron una frecuencia por debajo de una cota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_from_path(path: str, lower_bound=0, sortV = lambda list: None, log = lambda *_: None) -> tuple[str]:\n",
    "    lower_bound = max(lower_bound, 0)\n",
    "    with open(path) as file:\n",
    "        vocab = []\n",
    "        for line in file:\n",
    "            token, frecuency = line.strip().split(\",\", 1)\n",
    "            if isinstance(token, str) and frecuency.strip().isdigit(): # Format from .txt is token, frecuency\n",
    "                if token[-4:] == '</w>' or int(frecuency) >= lower_bound: # Filter sub-words tokens by lower bound\n",
    "                    vocab.append(tuple([token, int(frecuency)]))\n",
    "    sorted_vocab = sortV(vocab)\n",
    "    log(sorted_vocab, path, lower_bound)\n",
    "    return tuple(token for token,*_ in sorted_vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos dos funciones de ordenamiento auxiliares:\n",
    "- Por frecuencia: Nos sirve para saber que tokens aparecierons más veces en el texto.\n",
    "- Por nombre del token: Es el que usara nuestro ***encoder*** para reconocer el token más largo mientras lee el texto.\n",
    "\n",
    "Obtenemos la longitud de un token eliminando los últimos caractéres de `</w>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "def sort_by_frecuency(vocabulary: list):\n",
    "    return sorted(vocabulary, key = lambda item: item[1], reverse=True)\n",
    "\n",
    "def sort_by_token_name(vocabulary: list):\n",
    "    return sorted(vocabulary, key = lambda item: (measure_token_length(item[0]), item[1]), reverse=True) # Sort by lenght and then by alphabetic order.\n",
    "     \n",
    "def save_vocabulary(vocabulary, path, lower_bound):\n",
    "    filename = path.rsplit(\"/\")[-1]\n",
    "    path = os.path.join(\"vocabularios\", f\"from_{filename}_with_{lower_bound}_bound\")\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for token,*_ in vocabulary:\n",
    "            file.write(token + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos jugar con el parámetro de `lower_bound` para encontrar aquel que filtre tokens proveniente de sub-palabras que no fueron frecuentes en el algorítmo de byte-pair-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TOP---\n",
      "1 entretenimiento</w>\n",
      "2 verdaderamente</w>\n",
      "3 demasiadamente</w>\n",
      "4 principalmente</w>\n",
      "5 entendimiento.</w>\n",
      "---BOTTOM---\n",
      "6 s\n",
      "7 l\n",
      "8 g\n",
      "9 a\n",
      "10 </w>\n"
     ]
    }
   ],
   "source": [
    "v = get_vocab_from_path(path=\"outputs/no_pretokenization/vocab_quijote_8000.txt\", lower_bound=300, sortV= sort_by_token_name)\n",
    "\n",
    "# Sample from vocabulary v from top and bottom\n",
    "sample_size = 10\n",
    "bound = int(sample_size / 2)\n",
    "sample_v = []\n",
    "sample_v.extend(v[:bound])\n",
    "sample_v.extend(v[-bound:])\n",
    "print(\"---TOP---\")\n",
    "for i, token in enumerate(sample_v):\n",
    "    if i == bound:\n",
    "        print(\"---BOTTOM---\")\n",
    "    print(i+1, token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode de una palabra\n",
    "La razón por la que el vocabulario fue ordenado por la cantidad de caractéres de los tipos de tokens es porque para realizar el ***encoding*** se va iterar sobre el vocabulario $V$ para encontrar si el token $v_i = c_mc_{m+1}...c_k$ se encuentra en la cadena de caractéres $S = c_1...c_n$ y el ordenamiento asegura que encuentre el tipo de token más largo que capture a la cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_word(string, vocabulary, unknown_token='</u>') -> list:\n",
    "    if string == '':\n",
    "        return []\n",
    "    if vocabulary == []:\n",
    "        return [unknown_token]\n",
    "\n",
    "    string_tokens = []\n",
    "    for i in range(len(vocabulary)):\n",
    "        token = vocabulary[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "  \n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(string=substring, vocabulary=vocabulary[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, vocabulary=vocabulary[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "\n",
    "    if string_tokens == []:\n",
    "        return [unknown_token]\n",
    "    return string_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos tokenizar la palabra `facebook`, en la cual el `encoder` es capaz de reconocer los token encerrados en parentesis: `f(a)(c)ebook(<\\w>)` y dejando un `</u> ` para toda aquella cadena que sea ***oov*** (out of vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook</w> -> [encoder] -> ['</u>', 'a', 'c', '</u>', '</w>']\n"
     ]
    }
   ],
   "source": [
    "word = \"facebook</w>\"\n",
    "print( word, \"-> [encoder] ->\", tokenize_word(word, v, unknown_token='</u>'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode de una linea\n",
    "\n",
    "Definimos una función auxiliar para extender la tokenización de palabras a lineas, y después de lineas a archivos de .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line: str, vocabulary, unknown_token='</u>') -> list:\n",
    "    # Data Preprocessing\n",
    "    # Eliminamos espacios de más\n",
    "    line = \" \".join(line.split())\n",
    "\n",
    "    # Filtro de caractéres\n",
    "    line = line.translate({ord(i): None for i in \".;-\"})\n",
    "\n",
    "    # Convertimos/normalizamos el texto en el formato que usa nuestro encoder\n",
    "    line = line.replace(\" \", \"</w>\")\n",
    "    \n",
    "    return tokenize_word(line, vocabulary, unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Esto</w>', 'es</w>', 'una</w>', 'l', 'in', 'ea</w>', 'de</w>', '</u>', 'l', '</u>']\n"
     ]
    }
   ],
   "source": [
    "linea = \"Esto   es una linea  de ejemplo\"\n",
    "print(tokenize_line(linea, v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode de un archivo .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(filename: str, vocabulary, unknown_token='</u>') -> tuple:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        return tuple(token for line in file for token in tokenize_line(line, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = encode_text(\"corpus/extract.txt\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('</u>', '</w>', 'Juan</w>', '</u>', 'allo</w>', 'de</w>', '</u>', 'd', 'r', 'a', 'd', 'a', '</u>', '</w>', 'escribano</w>', 'de</w>', '</u>', 'ara</w>', 'del</w>', '</u>', 'y</w>', 'nuestro</w>', 's', '</u>', 'r', '</u>', '</w>', 'd', '</u>', 'los</w>', 'que</w>', 're', 's', '</u>', 'den</w>', 'en</w>', 'su</w>', '</u>', 's', '</u>', '</w>', 'c', 'er', 't', '</u>', 'co</w>', 'y</w>', 'doy</w>', 'fe</w>', '</u>', '</w>', 'habiendo</w>', 'visto</w>', '</u>', 'r', 'los</w>', 'señores</w>', 'dél</w>', 'un</w>', 'libro</w>', 'in', 't', '</u>', 't', '</u>', 'lado</w>', 'El</w>', 'ingenioso</w>', 'hidalgo</w>', 'de</w>', 'la</w>', '</u>', 'a', '</u>', 'c', '</u>', 'a', '</u>', 'compuesto</w>', 'por</w>', 'Miguel</w>', 'de</w>', 'Cervantes</w>', '</u>', 'a', 'a', '</u>', 'd', 'r', 'a', '</u>', '</w>', 't', 'a', 's', 'aron</w>', 'cada</w>', 'pliego</w>', 'del</w>', 'di', 'c', '</u>', 'libro</w>', 'a</w>', 'tres</w>', 'maravedís</w>', 'y</w>', 'medio</w>', 'el</w>', 'cual</w>', 'tiene</w>', '</u>', 'c', '</u>', 'enta</w>', 'y</w>', 'tres</w>', '</u>', 'l', '</u>', 'g', '</u>', 's', '</u>', '</w>', '</u>', 'al</w>', 'dicho</w>', 'precio</w>', '</u>', 'ta</w>', 'el</w>', 'dicho</w>', 'libro</w>', 'docientos</w>', 'y</w>', '</u>', 'venta</w>', 'maravedís</w>', 'y</w>', '</u>', 'di', '</u>', 'en</w>', 'que</w>', 'se</w>', 'ha</w>', 'de</w>', 'vender</w>', 'en</w>', 'papel</w>', 'y</w>', 'dieron</w>', 'licencia</w>', 'para</w>', 'que</w>', 'a</w>', 'este</w>', '</u>', 're', 'c', '</u>', 'se</w>', 'pueda</w>', '</u>', 'en', 'd', 'er', '</u>', '</w>', 'y</w>', '</u>', 'a', '</u>', 'd', 'aron</w>', 'que</w>', 'esta</w>', 't', 'a', 'sa</w>', 'se</w>', 'ponga</w>', 'al</w>', 'principio</w>', 'del</w>', 'di', 'c', '</u>', 'l', '</u>', 'r', '</u>', '</w>', 'y</w>', 'no</w>', 'se</w>', 'pueda</w>', 'vender</w>', 'sin</w>', 'ella</w>', '</u>', '</w>', 'para</w>', 'que</w>', 'dello</w>', 'c', '</u>', 's', 't', '</u>', '</w>', 'di</w>', 'l', 'a', 'presente</w>', 'en</w>', '</u>', 'al', 'l', 'a', 'd', '</u>', 'l', '</u>', 'd', '</u>', '</w>', 'a</w>', 'veinte</w>', 'días</w>', 'del</w>', 'mes</w>', 'de</w>', 'd', '</u>', 'c', '</u>', 'mbre</w>', 'de</w>', 'mil</w>', '</u>', 'seiscientos</w>', 'y</w>', 'cuatro</w>', 'a', '</u>', 's')\n"
     ]
    }
   ],
   "source": [
    "print(encoded_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas\n",
    "\n",
    "Proponemos medir el error de un vocabulario $V$ generado por aplicar el algorítmos de byte-pair-encoding. Sea $T$ los tokens generados por un ***encoder*** a un corpus $C$. El error $D$ será: $$D = \\dfrac{\\text{\\#OOV}}{|T|}$$\n",
    "\n",
    "Donde \\#OOV es la cantidad de tokens *out-of-vocabulary*, es decir, no reconocidos en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_in_vocab(tokens: tuple, unknown_token='</u>', log=lambda *_:None) -> float:\n",
    "    error = round(tokens.count(unknown_token) / len(tokens), 4)\n",
    "    log(error, tokens)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 248 tokens el 0.2258 son oov.\n"
     ]
    }
   ],
   "source": [
    "def error_stats(error, token):\n",
    "    n = len(token)\n",
    "    print(f\"De {n} tokens el {error} son oov.\")\n",
    "    \n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas\n",
    "\n",
    "Comparamos el error con la métrica definida arriba para el vocabulario obtenido del Quijote sobre 3 corpus: 1 es otro libro de Cervantes (el autor del Quijote), 1 libro de la epoca y un libro actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_no_pretokenization = get_vocab_from_path(path=\"outputs/no_pretokenization/vocab_quijote_8000.txt\", lower_bound=300, sortV= sort_by_token_name)\n",
    "vocab_pretokenization = get_vocab_from_path(path=\"outputs/pretokenization/vocab_quijote_8000.txt\", lower_bound=300, sortV= sort_by_token_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) La Gitanilla\n",
    "Novela corta de Miguel de Cervantes publicada en 1613.\n",
    "### Sin pretokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "# Computacionalmente caro: 52 min ...\n",
    "encoded_text = encode_text(\"corpus/extract_gitanilla.txt\", vocab_no_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 11585 tokens el 0.2631 son oov.\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el error: 0.2685\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "def save_encoding(tokens: tuple, filename: str):\n",
    "    with open(\"encodings/\" + filename, 'w', encoding='utf-8') as file:\n",
    "        for token in tokens:\n",
    "            file.write(token + \", \")\n",
    "    print(\"Saved.\")\n",
    "\n",
    "save_encoding(encoded_text, \"gitanilla.1.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con pretokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "# Computacionalmente caro: 52 min ...\n",
    "encoded_text = encode_text(\"corpus/extract_gitanilla.txt\", vocab_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 6717 tokens el 0.3189 son oov.\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el error: 0.2685\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "def save_encoding(tokens: tuple, filename: str):\n",
    "    with open(\"encodings/\" + filename, 'w', encoding='utf-8') as file:\n",
    "        for token in tokens:\n",
    "            file.write(token + \", \")\n",
    "    print(\"Saved.\")\n",
    "\n",
    "save_encoding(encoded_text, \"gitanilla.2.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fuente Ovejuna\n",
    "Novela de Lope de Vega, rival y amigo de Miguel de Cervantes (lo criticó en la primera parte del Quijote). Publicada en 1619\n",
    "### Sin pretokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "# Computacionalmente caro 26 min\n",
    "encoded_text = encode_text(\"corpus/extract_overjuna.txt\", vocab_no_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 5859 tokens el 0.3053 son oov.\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el error: 0.307\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "save_encoding(encoded_text, \"ovejuna.1.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con pretokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "# Computacionalmente caro 26 min\n",
    "encoded_text = encode_text(\"corpus/extract_overjuna.txt\", vocab_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 2999 tokens el 0.3918 son oov.\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el error: 0.307\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "save_encoding(encoded_text, \"ovejuna.2.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) El caballero encantado\n",
    "Novela de Benito Pérez Galdós, escrita en 1909."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "# Computacionalmente caro 96 min\n",
    "encoded_text = encode_text(\"corpus/extract_encantado.txt\", vocab_no_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el error: 0.2539\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoding(encoded_text, \"encantado.1.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos el corpus por el encoder\n",
    "encoded_text = encode_text(\"corpus/extract_encantado.txt\", vocab_pretokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el error: 0.2539\n",
    "error = error_in_vocab(encoded_text, log=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_encoding(encoded_text, \"encantado.2.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusiones**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumiendo los resultados obtenidos:\n",
    "\n",
    "| **Corpus** | **No pretokenization** | **pretokenization** |\n",
    "|------------|------------------------|---------------------|\n",
    "|**La Guitanilla**| 0.2631 | 0.3189 |\n",
    "| **Fuente ovejuna** | 0.3053  |   |\n",
    "|**El caballero encantado**|   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e572938b54010ecd00d17f54cb04110112cffdb0c89e43915cbb58e495b75ef7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
